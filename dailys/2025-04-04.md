## Understanding the Backward pass derivative for FA-3 Step-by-Step

### 1. What is Softmax?
$$
p_i = \frac{e^{s_i}}{\sum_{j=1}^N e^{s_j}}
$$

### 2. Why We Need Its Derivative
During backpropagation:
$$
\frac{\partial \phi}{\partial \mathbf{s}} = \frac{\partial \phi}{\partial \mathbf{p}} \cdot \frac{\partial \mathbf{p}}{\partial \mathbf{s}}
$$

Where:
- $\frac{\partial \phi}{\partial \mathbf{p}}$ is $dP$ (from next layer)
- $\frac{\partial \mathbf{p}}{\partial \mathbf{s}}$ is the Jacobian

### 3. Deriving the Softmax Jacobian

#### Case 1: Same Index ($i = j$)
$$
\frac{\partial p_i}{\partial s_i} = p_i(1 - p_i)
$$
- Note this is like sigmoid derivative

#### Case 2: Different Indices ($i \neq j$)
$$
\frac{\partial p_i}{\partial s_j} = -p_i p_j
$$

#### Combined Jacobian Matrix
$$
\frac{\partial \mathbf{p}}{\partial \mathbf{s}} = \text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^T
$$

### 4. Applying the Chain Rule
$$
d\mathbf{s} = d\mathbf{p} \cdot (\text{diag}(\mathbf{p}) - \mathbf{p}\mathbf{p}^T)
$$

What this does:
1. `diag(p)` scales gradients by probabilities
2. `-pp^T` redistributes gradients (competition)

### 5. Connection to Attention
In self-attention:
- Each row of $P$ gets this treatment
- $dS$ is computed row-wise from $dP$
- Used to find $dQ$ and $dK$
### Why This Matters
1. The Jacobian captures probability competition
2. diag(p) handles direct effects
3. -pp^T manages cross-effects
4. Each attention head computes this independently

## Derivation of dQ and dK

### Forward Pass
$$
S = \alpha \times (Q \times K^T)
$$
### Gradient for Q (dQ)

#### Chain Rule Application
$$
dQ = \frac{\partial \phi}{\partial Q} = \frac{\partial \phi}{\partial S} \times \frac{\partial S}{\partial Q}
$$

#### Derivative Calculation
Since:
$$
S = \alpha(QK^T)
$$
The derivative is:
$$
\frac{\partial S}{\partial Q} = \alpha K^T
$$

#### Final dQ Expression
$$
dQ = dS \times (\alpha K^T) = \alpha \times dS \times K
$$
- Note: We right-multiply by K instead of K^T due to matrix derivative rules

### Gradient for K (dK)

#### Chain Rule Application
$$
dK = \frac{\partial \phi}{\partial K} = \frac{\partial \phi}{\partial S} \times \frac{\partial S}{\partial K}
$$

#### Special Consideration
Because K appears as K^T in the forward pass, we need to transpose the gradient flow:
$$
\frac{\partial S}{\partial K} = \alpha Q
$$

#### Final dK Expression
$$
dK = \alpha \times dS^T \times Q
$$

### Key Implementation Notes
1. The $\alpha$ scaling factor must be preserved
2. For $dQ$: Multiply $dS$ by $K$ on the right
3. For $dK$: Transpose $dS$ first, then multiply by $Q$

### Why This Works
1. Matrix derivatives follow "reverse order" rule
2. Transpose appears because K was transposed in forward pass
3. $\alpha$ must be maintained for proper gradient scaling

## GPU Execution Model
### Memory Hierarchy
- GPU memory is organized as hierarchy of data locales, with capacity inversely related to bandwidth. 
- Global memory (GMEM), or HBM, is the DRAM which is mostly advertised and can be accessed by Streaming Multiprocessors (SMs). 
- Data from GMEM, gets transparently cached into an on-chip L2-cache.
- Each SM contains an on-chip highly banked cache called Shared Memory (SMEM), that is managed by the programmer.
- Register files exist in each SM.

### Thread Hierarchy
- Threads are local-groupings of execution units in the GPU.
- From the most fine to coarse, the hierarchy is:
	- Threads
	- Warps (32 Threads)
	- Warpgroups (4 contiguous warps)
	- Threadblocks (aka cooperative thread arrays or CTAs)
	- Threadblock clusters (in Hopper)
	- Grids
- Threads in the same CTA are co-scheduled on the same SM.
- CTAs in the same cluster are co-scheduled on the same GPC (Graphics processing Cluster) or Threadblock clusters.
- SMEM is directly addressable by all threads within a CTA.
- Each thread has at most 256 registers (RMEM) private to itself.

![[Thread-mem-hierarchy.png]]

### Asynchrony and Warp-specialization

GPUs are throughput processors that rely on concurrency and asynchronyto hide memory and execution latencies. For async memory copy between GMEM and SMEM, Hopper has theTensor Memory Accelerator (TMA) as a dedicated hardware unit. Furthermore, unlike prior architecturessuch as Ampere, the Tensor Core of Hopper, exposed via the warpgroup-wide WGMMA instruction, isalso asynchronous and can source its inputs directly from shared memory.

Hardware support for asynchrony allows for warp-specialized kernels, where the warps of a CTA are divided intoproducer or consumer roles that only ever issue either data movement or computation. Generically, this improvesthe compiler‚Äôs ability to generate optimal instruction schedules. In addition, Hopper supports the dynamicreallocation of registers between warpgroups via setmaxnreg, so those warps doing MMAs can obtaina larger share of RMEM than those just issuing TMA (for which only a single thread is needed).

### Low-precision number formats

GPUs have specialized hardware units for accelerating low-precisioncomputation. For example, the WGMMA instruction can target the FP8 Tensor Cores on Hopper to deliver 2x thethroughput per SM when compared to FP16 or BF16.

However, correctly invoking FP8 WGMMA entails understanding the layout constraints on its operands. Givena GEMM call to multiply ùê¥√ó ùêµ‚ä§for an ùëÄ√ó ùêæ-matrix ùê¥and an ùëÅ√ó ùêæ-matrix ùêµ, we say that the ùê¥or ùêµoperandis mn-major if it is contiguous in the outer ùëÄor ùëÅdimension, and k-major if is instead contiguous in the innerùêæ-dimension. Then for FP16 WGMMA, both mn-major and k-major input operands are accepted for operands inSMEM, but for FP8 WGMMA, only the k-major format is supported. Moreover, in situations such as attentionwhere one wants to fuse back-to-back GEMMs in a single kernel, clashing FP32 accumulator and FP8 operandlayouts pose an obstacle to invoking dependent FP8 WGMMAs.